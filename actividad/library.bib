Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{McGuire2014,
abstract = {The sudoku minimum number of clues problem is the following question: what is the smallest number of clues that a sudoku puzzle can have? For several years it had been conjectured that the answer is 17. We have performed an exhaustive computer search for 16-clue sudoku puzzles, and did not find any, thus proving that the answer is indeed 17. In this article we describe our method and the actual search. As a part of this project we developed a novel way for enumerating hitting sets. The hitting set problem is computationally hard; it is one of Karp's 21 classic NP-complete problems. A standard backtracking algorithm for finding hitting sets would not be fast enough to search for a 16-clue sudoku puzzle exhaustively, even at today's supercomputer speeds. To make an exhaustive search possible, we designed an algorithm that allowed us to efficiently enumerate hitting sets of a suitable size.},
archivePrefix = {arXiv},
arxivId = {1201.0749},
author = {McGuire, Gary and Tugemann, Bastian and Civario, Gilles},
doi = {10.1080/10586458.2013.870056},
eprint = {1201.0749},
file = {:Users/pablohidalgo/Downloads/McGuire{\_}V1.pdf:pdf},
issn = {1944950X},
journal = {Experimental Mathematics},
keywords = {algorithm,critical sets,hitting set problem,minimum number of clues,sudoku},
number = {2},
pages = {190--217},
title = {{There Is No 16-Clue sudoku: Solving the sudoku minimum number of clues problem via hitting set enumeration}},
volume = {23},
year = {2014}
}
@article{Stamp2004,
abstract = {Suppose we want to determine the average annual temperature at a particular location on earth over a series of years. To make it interesting, suppose the years we are concerned with lie in the distant past, before thermometers were invented. Since we can't go back in time, we instead look for indirect evidence of the temperature. To simplify the problem, we only consider two annual temperatures, " hot " and " cold " . Suppose that modern evidence indicates that the probability of a hot year followed by another hot year is 0.7 and the probability that a cold year is followed by another cold year is 0.6. We'll assume that these probabilities held in the distant past as well. The information so far can be summarized as H C H C 0.7 0.3 0.4 0.6 (1) where H is " hot " and C is " cold " . Also suppose that current research indicates a correlation between the size of tree growth rings and temperature. For simplicity, we only consider three different tree ring sizes, small, medium and large, or S, M and L, respectively. Finally, suppose that based on available evidence, the probabilistic relationship between annual temperature and tree ring sizes is given by S M L H C 0.1 0.4 0.5 0.7 0.2 0.1},
author = {Stamp, Mark},
doi = {10.1.1.136.137},
file = {:Users/pablohidalgo/Library/Application Support/Mendeley Desktop/Downloaded/Stamp - 2004 - A revealing introduction to hidden Markov models.pdf:pdf},
journal = {Department of Computer Science San Jose State {\ldots}},
keywords = {HMM},
mendeley-tags = {HMM},
pages = {1--20},
title = {{A revealing introduction to hidden Markov models}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Revealing+Introduction+to+Hidden+Markov+Models{\#}0},
year = {2004}
}
@article{Lynce2006,
abstract = {Sudoku is a very simple and well-known puzzle that has achieved international popularity in the recent past. This paper addresses the problem of encoding Sudoku puzzles into conjunctive normal form (CNF), and subsequently solving them using polynomial-time propositional satisfiability (SAT) inference techniques. We introduce two straightforward SAT encodings for Sudoku: the minimal encoding and the extended encoding. The minimal encoding suffices to characterize Sudoku puzzles, whereas the extended encoding adds redundant clauses to the minimal encoding. Experimental results demonstrate that, for thousands of very hard puzzles, inference techniques struggle to solve these puzzles when using the minimal encoding. However, using the extended encoding, unit propagation is able to solve about half of our set of puzzles. Nonetheless, for some puzzles more sophisticated inference techniques are required.},
author = {Lynce, I and Ouaknine, J},
file = {:Users/pablohidalgo/Downloads/sudokusat.pdf:pdf},
journal = {Symposium A Quarterly Journal In Modern Foreign Literatures},
pages = {1--9},
title = {{Sudoku as a SAT Problem}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.6274{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@article{Qi2014,
abstract = {Accurate short-term prediction of traffic conditions on freeways and major arterials has recently become increasingly important because of its vital role in the basic traffic management functions and trip decision making processes. Given the dynamic and stochastic nature of freeway traffic, this study proposes a stochastic approach, Hidden Markov Model (HMM), for short-term freeway traffic prediction during peak periods. The data used in the study was gathered from real-time traffic monitoring devices over six years on a 60.8-km (38-mile) corridor of Interstate-4 in Orlando, Florida. The HMM defines traffic states in a two-dimensional space using first-order statistics (Mean) and second-order statistics (Contrast) of speed observations. The dynamic changes of freeway traffic conditions are addressed with state transition probabilities. For a sequence of traffic speed observations, HMMs estimate the most likely sequence of traffic states. The model performance was evaluated using prediction errors, which are measured by the relative length of the distance between the predicted state and the observed state in the two-dimensional space. Reasonable prediction errors lower than or around 10{\%} were obtained from HMMs. Also, the model performance was not remarkably affected by location, travel direction, and peak period time. The HMMs were compared to two na{\"{i}}ve predication methods. The results showed that HMMs perform better and are more robust than the na{\"{i}}ve methods. Therefore, the study concludes that the HMM approach was successful in modeling short-term traffic condition prediction during peak periods and in accounting for the inherent stochastic nature of traffic conditions. {\textcopyright} 2014.},
author = {Qi, Yan and Ishak, Sherif},
doi = {10.1016/j.trc.2014.02.007},
file = {:Users/pablohidalgo/Library/Application Support/Mendeley Desktop/Downloaded/Qi, Ishak - 2014 - A Hidden Markov Model for short term prediction of traffic conditions on freeways.pdf:pdf},
isbn = {0968-090X},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {Clustering analysis,Freeway traffic,HMM,Hidden Markov Models,Second-order statistics,Short-term traffic prediction,Stochastic modeling,Transition probability},
mendeley-tags = {HMM},
pages = {95--111},
publisher = {Elsevier Ltd},
title = {{A Hidden Markov Model for short term prediction of traffic conditions on freeways}},
url = {http://dx.doi.org/10.1016/j.trc.2014.02.007},
volume = {43},
year = {2014}
}
@book{Hastie2001,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {The Mathematical Intelligencer},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:Users/pablohidalgo/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2001 - The Elements of Statistical Learning.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
keywords = {inger series in statistics},
number = {2},
pages = {83--85},
pmid = {21196786},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf{\%}255Cnhttp://www-stat.stanford.edu/{~}tibs/book/preface.ps},
volume = {27},
year = {2001}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:Users/pablohidalgo/Library/Application Support/Mendeley Desktop/Downloaded/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {LDA,Text mining},
mendeley-tags = {LDA,Text mining},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{Rabin1959,
abstract = {Finite automata are considered in this paper as instruments for classifying finite tapes. Each one-tape automaton defines a set of tapes, a two-tape automaton defines a set of pairs of tapes, et cetera. The structure of the defined sets is studied. Various generalizations of the notion of an automaton are introduced and their relation to the classical automata is determined. Some decision problems concerning automata are shown to be solvable by effective algorithms; others turn out to be unsolvable by algorithms.},
author = {Rabin, Michael O and Scott, Dana},
doi = {10.1147/rd.32.0114},
file = {:Users/pablohidalgo/Library/Application Support/Mendeley Desktop/Downloaded/Rabin, Scott - 1959 - Finite Automata and Their Decision Problems.pdf:pdf},
isbn = {9781424438884},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
number = {2},
pages = {114--125},
title = {{Finite Automata and Their Decision Problems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5392601},
volume = {3},
year = {1959}
}
@article{Gneiting2011,
abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that t...},
archivePrefix = {arXiv},
arxivId = {0912.0902},
author = {Gneiting, Tilmann},
doi = {10.1198/jasa.2011.r10138},
eprint = {0912.0902},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes rule,Bregman function,Conditional value-at-risk (CVaR),Decision theory,Elicitability,Expectile,MAPE,Mean,Median,Mode,Proper scoring rule,Quantile,Statistical functional},
mendeley-tags = {MAPE},
number = {494},
pages = {746--762},
title = {{Making and Evaluating Point Forecasts}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.r10138},
volume = {106},
year = {2011}
}
